{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting content from def_text\n",
    "def split_text(row):\n",
    "    string_data = row['def_text']\n",
    "\n",
    "    split_columns = [\n",
    "        'PscInspectionId', 'Deficiency/Finding', 'Description Overview', 'Immediate Causes',\n",
    "        'Root Cause Analysis', 'Corrective Action', 'Preventive Action', 'Deficiency Code',\n",
    "        'Detainable Deficiency'\n",
    "    ]\n",
    "\n",
    "    pattern = r\"(\" + \"|\".join(re.escape(col) for col in split_columns) + r\"): (.*?)(?=\\n[A-Z]|$)\"\n",
    "    matches = re.findall(pattern, string_data, re.DOTALL)\n",
    "\n",
    "    result = {col: \"\" for col in split_columns}\n",
    "    for key, value in matches:\n",
    "        result[key] = value.strip()\n",
    "\n",
    "    return [result[col] for col in split_columns]\n",
    "\n",
    "data_type = \"train\"\n",
    "data = pd.read_csv(f\"../../data/dataset/psc_severity_{data_type}.csv\")\n",
    "split_columns = [\n",
    "        'PscInspectionId', 'Deficiency/Finding', 'Description Overview', 'Immediate Causes',\n",
    "        'Root Cause Analysis', 'Corrective Action', 'Preventive Action', 'Deficiency Code',\n",
    "        'Detainable Deficiency'\n",
    "    ]\n",
    "split_results = data.apply(split_text, axis=1)\n",
    "data[split_columns] = pd.DataFrame(split_results.to_list(), index=data.index)\n",
    "\n",
    "## Removing unnessary columns\n",
    "clean_data = data[['PscInspectionId', 'InspectionDate', 'VesselId',\n",
    "       'PscAuthorityId', 'PortId', 'VesselGroup', 'age',\n",
    "       'Deficiency/Finding', 'Description Overview', 'Immediate Causes',\n",
    "       'Root Cause Analysis', 'Corrective Action', 'Preventive Action',\n",
    "       'Deficiency Code', 'Detainable Deficiency']]\n",
    "\n",
    "## Saving the cleaned data\n",
    "clean_data.to_csv(f\"../../data/dataset/cleaned_{data_type}_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Using Generative AI to get 'Second unbiased opinion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"train\"\n",
    "data = pd.read_csv(f\"../data/dataset/cleaned_{data_type}_data.csv\")\n",
    "data['Deficiency Code'] = data['Deficiency Code'].astype(str).str.zfill(5)\n",
    "\n",
    "## Get unique inspection records\n",
    "grouped_data = data.groupby([ 'Deficiency Code',\n",
    "       'PscAuthorityId', 'VesselGroup', 'Deficiency/Finding',\n",
    "       'Description Overview', 'Immediate Causes', 'Root Cause Analysis',\n",
    "       'Corrective Action', 'Preventive Action',\n",
    "       'Detainable Deficiency',\"age\",'PortId']).mean().round(0).astype(int).reset_index()\n",
    "\n",
    "## Open-source LLM\n",
    "model_name = \"google/flan-t5-large\"  \n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "\n",
    "## Prompt Engineering to get second opinion\n",
    "severity_labels = ['Low', 'Medium', 'High']\n",
    "processed = 1\n",
    "low_count = 0\n",
    "medium_count = 0\n",
    "high_count = 0\n",
    "total = grouped_data.shape[0]\n",
    "\n",
    "def generate_severity_label(row, severity_labels, generator, max_new_tokens=10, temperature=0.8, top_k=50, top_p=0.95):\n",
    "    global processed \n",
    "    global total\n",
    "    global low_count\n",
    "    global medium_count\n",
    "    global high_count\n",
    "    prompt = (\n",
    "        \"You are a Port State Control (PSC) inspector evaluating the severity of deficiencies. \"\n",
    "        \"Based on the input parameters below, respond only with the severity rating: Low, Medium, or High. \"\n",
    "        \"Do not include any additional text.\\n\\nThink step-by-step before providing the severity rating.\\n\\n\"\n",
    "        \"Input Parameters:\\n\"\n",
    "        f\"- Deficiency Code: {row['Deficiency Code']}\\n\"\n",
    "        f\"- PSC Authority ID: {row['PscAuthorityId']}\\n\"\n",
    "        f\"- Port ID: {row['PortId']}\\n\"\n",
    "        f\"- Vessel Group: {row['VesselGroup']}\\n\"\n",
    "        f\"- Age: {row['age']}\\n\"\n",
    "        f\"- Deficiency/Finding: {row['Deficiency/Finding']}\\n\"\n",
    "        f\"- Description Overview: {row['Description Overview']}\\n\"\n",
    "        f\"- Immediate Causes: {row['Immediate Causes']}\\n\"\n",
    "        f\"- Root Cause Analysis: {row['Root Cause Analysis']}\\n\"\n",
    "        f\"- Corrective Action: {row['Corrective Action']}\\n\"\n",
    "        f\"- Preventive Action: {row['Preventive Action']}\\n\"\n",
    "        f\"- Detainable Deficiency: {row['Detainable Deficiency']}\\n\\n\"\n",
    "        \"Severity Rating:\"\n",
    "    )\n",
    "    print(f\"Processing rows {processed}/{total}\")\n",
    "    processed += 1\n",
    "    try:\n",
    "        generated = generator(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "        generated_text = generated[0]['generated_text'].strip()\n",
    "        print(f\"Generated Text: {generated_text}\")  \n",
    "\n",
    "        # Extract the severity label using regex for reliability\n",
    "        match = re.search(r'\\b(Low|Medium|High)\\b', generated_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            label = match.group(1).capitalize()\n",
    "            if label in severity_labels:\n",
    "                if label == \"Low\":\n",
    "                    low_count += 1\n",
    "                if label == \"Medium\":\n",
    "                    medium_count += 1\n",
    "                if label == \"High\":\n",
    "                    high_count += 1\n",
    "                print(f\"Extracted Label: {label}\")  \n",
    "                print(low_count,medium_count,high_count)\n",
    "                return label\n",
    "    \n",
    "        print(\"Extraction Failed: Defaulting to 'Medium'\")\n",
    "        medium_count += 1\n",
    "        return 'Medium'  # Default label\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating severity label for row {row.name}: {e}\")\n",
    "        return 'Medium'\n",
    "    \n",
    "grouped_data['synthetic_severity'] = grouped_data.apply(\n",
    "    lambda x: generate_severity_label(x, severity_labels, generator),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "## Saved second opinion data\n",
    "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
    "grouped_data['synthetic_severity_encoded'] = encoder.fit_transform(grouped_data[['synthetic_severity']]).astype(int)\n",
    "grouped_data.to_csv('./augmented_data2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train AutoML model: AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"train\"\n",
    "data = pd.read_csv(f\"../../data/dataset/cleaned_{data_type}_data.csv\")\n",
    "augmented_data = pd.read_csv(f\"../augmented_data.csv\")\n",
    "augmented_data = augmented_data.drop(columns=[\"severity_encoded\", \"synthetic_severity\"])\n",
    "augmented_data = augmented_data.rename(columns={\"synthetic_severity_encoded\": \"severity_encoded\"})\n",
    "data = pd.concat([data, augmented_data])\n",
    "\n",
    "data = data[['Deficiency Code','age', 'PscAuthorityId', 'VesselGroup', 'Deficiency/Finding', 'Description Overview', \n",
    "                'Immediate Causes', 'Root Cause Analysis', 'Corrective Action', \n",
    "                'Preventive Action', 'Detainable Deficiency', 'severity_encoded']]\n",
    "data['Deficiency Code'] = data['Deficiency Code'].astype(str).str.zfill(5)\n",
    "\n",
    "\n",
    "grouped_data = data.groupby([ 'Deficiency Code',\n",
    "       'PscAuthorityId', 'VesselGroup', 'Deficiency/Finding',\n",
    "       'Description Overview', 'Immediate Causes', 'Root Cause Analysis',\n",
    "       'Corrective Action', 'Preventive Action',\n",
    "       'Detainable Deficiency',\"age\"]).mean().round(0).astype(int).reset_index()\n",
    "\n",
    "\n",
    "metric = \"roc_auc_ovo\"\n",
    "target = 'severity_encoded'\n",
    "\n",
    "train_data, val_data = train_test_split(grouped_data, test_size=0.3, random_state=99)\n",
    "\n",
    "train_data = TabularDataset(train_data)\n",
    "val_data = TabularDataset(val_data)\n",
    "\n",
    "quality = 'medium_quality' #Lowest size model used due to computer power restriction\n",
    "predictor = TabularPredictor(label=target,eval_metric=metric).fit(train_data,presets=quality)  \n",
    "y_pred = predictor.predict(val_data.drop(columns=[target]))\n",
    "\n",
    "\n",
    "print(predictor.evaluate(val_data))\n",
    "print(predictor.leaderboard(val_data))\n",
    "print(predictor.feature_importance(val_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
